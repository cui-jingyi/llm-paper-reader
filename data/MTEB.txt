MTEB: Massive Text Embedding Benchmark Niklas Muennighoff1, Nouamane Tazi1, Loïc Magne1, Nils Reimers2* 1Hugging Face 2cohere.ai 1firstname@hf.co 2info@nils-reimers.de Abstract Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art em- beddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the ﬁeld difﬁcult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we intro- duce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks cov- ering a total of 58 datasets and 112 languages.

Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date.

We ﬁnd that no particular text embedding method dominates across all tasks. This suggests that the ﬁeld has yet to converge on a universal text embedding method and scale it up sufﬁciently to provide state-of-the-art results on all embed- ding tasks.

MTEB comes with open-source code and a public leaderboard at https: //github.com/embeddings-benchm ark/mteb.

1 Introduction Natural language embeddings power a variety of use cases from clustering and topic representa- tion (Aggarwal and Zhai, 2012; Angelov, 2020) to search systems and text mining (Huang et al., 2020; Zhu et al., 2021; Nayak, 2019) to feature representations for downstream models (Saharia et al., 2022; Borgeaud et al., 2022). Using gener- ative language models or cross-encoders for these applications is often intractable, as they may re- quire exponentially more computations (Reimers and Gurevych, 2019).

However, the evaluation regime of current text embedding models rarely covers the breadth of *Most of the work done while at Hugging Face. Corre- spondence to n.muennighoff@gmail.com. their possible use cases.

For example, Sim- CSE (Gao et al., 2021b) or SBERT (Reimers and Gurevych, 2019) solely evaluate on STS and clas- siﬁcation tasks, leaving open questions about the transferability of the embedding models to search or clustering tasks. STS is known to poorly corre- late with other real-world use cases (Neelakantan et al., 2022; Wang et al., 2021). Further, evaluating embedding methods on many tasks requires imple- menting multiple evaluation pipelines. Implemen- tation details like pre-processing or hyperparam- eters may inﬂuence the results making it unclear whether performance improvements simply come from a favorable evaluation pipeline. This leads to the “blind” application of these models to new use cases in industry or requires incremental work to reevaluate them on different tasks.

The Massive Text Embedding Benchmark (MTEB) aims to provide clarity on how models perform on a variety of embedding tasks and thus serves as the gateway to ﬁnding universal text em- beddings applicable to a variety of tasks. MTEB consists of 58 datasets covering 112 languages from 8 embedding tasks: Bitext mining, classi- ﬁcation, clustering, pair classiﬁcation, reranking, retrieval, STS and summarization. MTEB software is available open-source1 enabling evaluation of any embedding model by adding less than 10 lines of code. Datasets and the MTEB leaderboard are available on the Hugging Face Hub2.

We evaluate over 30 models on MTEB with addi- tional speed and memory benchmarking to provide a holistic view of the state of text embedding mod- els. We cover both models available open-source as well as models accessible via APIs, such as the OpenAI Embeddings endpoint. We ﬁnd there to be no single best solution, with different models dom- 1https://github.com/embeddings-benchm ark/mteb 2https://huggingface.co/spaces/mteb/l eaderboard arXiv:2210.07316v3  [cs.CL]  19 Mar 2023 inating different tasks. Our benchmarking sheds light on the weaknesses and strengths of individual models, such as SimCSE’s (Gao et al., 2021b) low performance on clustering and retrieval despite its strong performance on STS. We hope our work makes selecting the right embedding model easier and simpliﬁes future embedding research.

2 Related Work 2.1 Benchmarks Benchmarks, such as (Super)GLUE (Wang et al., 2018, 2019) or Big-BENCH (Srivastava et al., 2022), and evaluation frameworks (Gao et al., 2021a) play a key role in driving NLP progress.

Yearly released SemEval datasets (Agirre et al., 2012, 2013, 2014, 2015, 2016) are commonly used as the go-to benchmark for text embeddings. Se- mEval datasets correspond to the task of semantic textual similarity (STS) requiring models to embed similar sentences with geometrically close embed- dings. Due to the limited expressivity of a single Se- mEval dataset, SentEval (Conneau and Kiela, 2018) aggregates multiple STS datasets. SentEval focuses on ﬁne-tuning classiﬁers on top of embeddings. It lacks tasks like retrieval or clustering, where em- beddings are directly compared without additional classiﬁers. Further, the toolkit was proposed in 2018 and thus does not provide easy support for recent trends like text embeddings from transform- ers (Reimers and Gurevych, 2019). Due to the insufﬁciency of STS benchmarking, USEB (Wang et al., 2021) was introduced consisting mostly of reranking tasks. Consequently, it does not cover tasks like retrieval or classiﬁcation. Meanwhile, the recently released BEIR Benchmark (Thakur et al., 2021) has become the standard for the evaluation of embeddings for zero-shot information retrieval.

MTEB uniﬁes datasets from different embed- ding tasks into a common, accessible evaluation framework. MTEB incorporates SemEval datasets (STS11 - STS22) and BEIR alongside a variety of other datasets from various tasks to provide a holis- tic performance review of text embedding models.

2.2 Embedding Models Text embedding models like Glove (Pennington et al., 2014) lack context awareness and are thus commonly labeled as Word Embedding Models.

They consist of a layer mapping each input word to a vector often followed by an averaging layer to provide a ﬁnal embedding invariant of input length.

Transformers (Vaswani et al., 2017) inject context awareness into language models via self-attention and form the foundation of most recent embed- ding models. BERT (Devlin et al., 2018) uses the transformer architecture and performs large-scale self-supervised pre-training. The resulting model can directly be used to produce text embeddings via an averaging operation alike Glove. Build- ing on InferSent (Conneau et al., 2017), SBERT (Reimers and Gurevych, 2019) demonstrated it to be beneﬁcial to perform additional ﬁne-tuning of the transformer for competitive embedding perfor- mance. Most recent ﬁne-tuned embedding models use a contrastive loss objective to perform super- vised ﬁne-tuning on positive and negative text pairs (Gao et al., 2021b; Wang et al., 2021; Ni et al., 2021b; Muennighoff, 2022). Due to the large va- riety of available pre-trained transformers (Wolf et al., 2020), there is an at least equally large va- riety of potential text embedding models to be ex- plored. This leads to confusion about which model provides practitioners with the best performance for their embedding use case.

We benchmark both word embedding and trans- former models on MTEB quantifying gains pro- vided by often much slower context aware models.

3 The MTEB Benchmark 3.1 Desiderata MTEB is built on a set of desiderata: (a) Diversity: MTEB aims to provide an understanding of the usability of embedding models in various use cases.

The benchmark comprises 8 different tasks, with up to 15 datasets each. Of the 58 total datasets in MTEB, 10 are multilingual, covering 112 differ- ent languages. Sentence-level and paragraph-level datasets are included to contrast performance on short and long texts. (b) Simplicity: MTEB pro- vides a simple API for plugging in any model that given a list of texts can produce a vector for each list item with a consistent shape. This makes it possible to benchmark a diverse set of models. (c) Extensibility: New datasets for existing tasks can be benchmarked in MTEB via a single ﬁle that speciﬁes the task and a Hugging Face dataset name where the data has been uploaded (Lhoest et al., 2021). New tasks require implementing a task in- terface for loading the data and an evaluator for benchmarking. We welcome dataset, task or metric contributions from the community via pull requests to continue the development of MTEB. (d) Repro- MTEB 8 Tasks 58 Datasets Massive Text   Embedding Benchmark Classification AmazonCounterfactual Retrieval Pair Classification AmazonPolarity AmazonReviews Banking77 Emotion Imdb MassiveIntent MassiveScenario MTOPDomain MTOPIntent ToxicConversations TweetSentimentExtraction SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus Clustering ArxivP2P ArxivS2S STS BIOSESS SICK-R STS11 STS12 STS13 STS14 Reranking Summarization STS15 STS16 AskUbuntuDupQuestions MindSmallReranking SciDocsRR StackOverFlowDupQuestions SummEval  STSB STS17 STS22 ArguAna ClimateFEVER CQADupstackRetrieval FEVER DBPedia FiQA2018 HotpotQA MSMARCO NFCorpus NQ Quora SCIDOCS SciFact Touche2020 TRECCOVID MedrxivP2P MedrxivS2S Reddit StackExchange RedditP2P StackExchangeP2P TwentyNewsgroup BiorxivP2P BiorxivS2S Bitext Mining BUCC Tatoeba Figure 1: An overview of tasks and datasets in MTEB. Multilingual datasets are marked with a purple shade. ducibility: Through versioning at a dataset and software level, we aim to make it easy to repro- duce results in MTEB. JSON ﬁles corresponding to all results available in this paper have been made available together with the MTEB benchmark3.

3.2 Tasks and Evaluation Figure 1 provides an overview of tasks and datasets available in MTEB. Dataset statistics are available in Table 2. The benchmark consists of the follow- ing 8 task types: Bitext Mining Inputs are two sets of sentences from two different languages. For each sentence in the ﬁrst set, the best match in the second set needs to be found. The matches are commonly translations. The provided model is used to embed each sentence and the closest pairs are found via cosine similarity. F1 serves as the main metric for bitext mining. Accuracy, precision and recall are also computed.

Classiﬁcation A train and test set are embedded with the provided model. The train set embeddings are used to train a logistic regression classiﬁer with 100 maximum iterations, which is scored on the test set. The main metric is accuracy with average precision and f1 additionally provided.

3https://huggingface.co/datasets/mteb /results Clustering Given a set of sentences or para- graphs, the goal is to group them into meaning- ful clusters. A mini-batch k-means model with batch size 32 and k equal to the number of dif- ferent labels (Pedregosa et al., 2011) is trained on the embedded texts. The model is scored using v-measure (Rosenberg and Hirschberg, 2007). V- measure does not depend on the cluster label, thus the permutation of labels does not affect the score.

Pair Classiﬁcation A pair of text inputs is pro- vided and a label needs to be assigned. Labels are typically binary variables denoting duplicate or paraphrase pairs. The two texts are embedded and their distance is computed with various metrics (cosine similarity, dot product, euclidean distance, manhattan distance). Using the best binary thresh- old accuracy, average precision, f1, precision and recall are computed. The average precision score based on cosine similarity is the main metric.

Reranking Inputs are a query and a list of rele- vant and irrelevant reference texts. The aim is to rank the results according to their relevance to the query. The model is used to embed the references which are then compared to the query using cosine similarity. The resulting ranking is scored for each query and averaged across all queries. Metrics are mean MRR@k and MAP with the latter being the main metric.

AmazonCounterfactualClassification AmazonPolarityClassification AmazonReviewsClassification Banking77Classification EmotionClassification ImdbClassification MassiveIntentClassification MassiveScenarioClassification MTOPDomainClassification MTOPIntentClassification ToxicConversationsClassification TweetSentimentExtractionClassification ArxivClusteringP2P ArxivClusteringS2S BiorxivClusteringP2P BiorxivClusteringS2S MedrxivClusteringP2P MedrxivClusteringS2S RedditClustering RedditClusteringP2P StackExchangeClustering StackExchangeClusteringP2P TwentyNewsgroupsClustering SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus AskUbuntuDupQuestions MindSmallReranking SciDocsRR StackOverflowDupQuestions ArguAna ClimateFEVER CQADupstackAndroidRetrieval CQADupstackEnglishRetrieval CQADupstackGamingRetrieval CQADupstackGisRetrieval CQADupstackMathematicaRetrieval CQADupstackPhysicsRetrieval CQADupstackProgrammersRetrieval CQADupstackStatsRetrieval CQADupstackTexRetrieval CQADupstackUnixRetrieval CQADupstackWebmastersRetrieval CQADupstackWordpressRetrieval DBPedia FEVER FiQA2018 HotpotQA MSMARCO NFCorpus NQ QuoraRetrieval SCIDOCS SciFact Touche2020 TRECCOVID BIOSSES SICK-R STS12 STS13 STS14 STS15 STS16 STS17 STS22 STSBenchmark SummEval AmazonCounterfactualClassification AmazonPolarityClassification AmazonReviewsClassification Banking77Classification EmotionClassification ImdbClassification MassiveIntentClassification MassiveScenarioClassification MTOPDomainClassification MTOPIntentClassification ToxicConversationsClassification TweetSentimentExtractionClassification ArxivClusteringP2P ArxivClusteringS2S BiorxivClusteringP2P BiorxivClusteringS2S MedrxivClusteringP2P MedrxivClusteringS2S RedditClustering RedditClusteringP2P StackExchangeClustering StackExchangeClusteringP2P TwentyNewsgroupsClustering SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus AskUbuntuDupQuestions MindSmallReranking SciDocsRR StackOverflowDupQuestions ArguAna ClimateFEVER CQADupstackAndroidRetrieval CQADupstackEnglishRetrieval CQADupstackGamingRetrieval CQADupstackGisRetrieval CQADupstackMathematicaRetrieval CQADupstackPhysicsRetrieval CQADupstackProgrammersRetrieval CQADupstackStatsRetrieval CQADupstackTexRetrieval CQADupstackUnixRetrieval CQADupstackWebmastersRetrieval CQADupstackWordpressRetrieval DBPedia FEVER FiQA2018 HotpotQA MSMARCO NFCorpus NQ QuoraRetrieval SCIDOCS SciFact Touche2020 TRECCOVID BIOSSES SICK-R STS12 STS13 STS14 STS15 STS16 STS17 STS22 STSBenchmark SummEval 97 85 84 90 89 83 90 89 84 87 91 94 81 85 85 92 92 89 91 89 88 92 92 89 91 89 88 100 91 92 87 92 88 88 98 98 91 92 87 92 88 88 98 98 100 93 93 87 90 89 90 96 96 95 95 94 94 89 91 92 90 97 97 96 96 98 91 91 83 87 83 86 90 90 89 89 89 89 92 93 87 90 87 89 97 97 95 95 96 96 93 88 88 81 85 82 85 87 87 87 87 87 87 95 90 91 91 85 87 85 88 93 93 92 92 92 92 95 96 94 87 87 81 84 81 84 87 87 87 87 87 87 92 89 96 93 89 89 83 85 83 85 90 90 89 89 89 90 93 93 93 97 96 94 94 88 92 90 89 95 95 95 95 95 96 90 95 88 93 88 91 94 95 86 93 92 91 95 95 95 95 96 97 92 95 90 93 89 91 96 92 92 89 91 88 88 95 95 94 94 94 94 92 96 89 94 89 92 95 95 87 87 79 86 82 83 90 90 89 89 89 88 89 91 86 90 85 88 89 91 92 93 93 88 91 87 88 96 96 95 95 95 96 92 98 89 95 90 93 95 95 96 91 74 74 69 78 72 69 77 77 79 79 74 75 73 75 71 75 71 74 77 76 77 74 77 88 89 83 85 85 85 91 91 90 90 92 92 85 91 83 88 83 85 91 92 89 84 90 71 92 92 84 88 87 89 92 92 92 92 93 93 89 92 87 91 87 89 93 93 92 86 93 74 88 88 87 85 89 84 84 92 92 91 91 89 91 89 91 86 90 85 88 90 90 92 88 92 77 85 87 84 86 80 81 80 84 89 89 88 88 88 88 85 88 82 88 82 86 88 88 87 82 89 67 84 88 83 91 92 86 89 85 88 95 95 93 93 93 93 94 97 91 97 91 95 94 94 95 92 96 76 89 92 91 89 88 88 84 87 83 83 92 92 91 91 89 90 90 92 86 92 86 90 90 90 93 92 92 75 85 87 92 84 93 92 91 84 87 85 89 90 90 90 90 91 90 92 91 91 91 91 90 91 93 92 87 92 72 86 91 87 86 92 88 87 88 83 86 83 84 91 91 90 90 90 91 88 91 85 90 86 88 91 90 90 85 92 72 85 88 86 84 90 86 87 88 87 80 89 82 84 90 90 90 90 88 88 87 89 85 88 85 86 89 90 91 92 90 79 84 86 90 81 90 90 87 85 91 91 83 89 86 87 92 92 92 92 92 92 90 93 88 91 88 89 93 93 96 91 93 74 86 90 87 83 92 89 91 88 91 91 90 82 90 85 87 93 93 92 92 91 91 89 93 87 91 87 89 93 94 94 95 93 75 87 89 90 84 92 91 90 88 94 94 86 85 79 86 80 81 87 87 87 87 86 86 88 89 85 89 86 88 87 88 91 93 89 74 81 84 88 80 90 91 86 84 91 90 92 88 87 80 87 82 83 89 89 89 89 87 87 91 91 88 91 87 89 89 90 93 94 90 77 82 86 89 81 92 92 88 85 92 92 93 94 88 88 80 87 82 83 89 89 88 88 89 88 93 92 88 92 87 89 90 91 93 92 91 73 83 87 87 82 92 88 90 86 91 94 93 90 93 88 88 81 87 82 85 90 90 89 89 88 88 90 91 88 91 87 89 90 91 94 95 92 75 83 87 88 81 93 92 91 85 92 95 94 93 94 94 87 87 80 86 81 82 88 88 88 88 87 87 92 91 89 92 90 91 89 90 92 93 91 74 83 86 87 81 93 90 89 85 90 93 92 93 96 94 95 87 87 80 86 80 82 88 88 88 88 86 87 90 90 87 90 86 88 89 89 93 92 90 75 82 85 88 81 90 91 87 84 90 92 91 92 96 91 93 93 88 87 81 88 82 84 90 90 89 89 88 88 89 90 87 90 86 88 90 90 93 93 91 76 83 86 93 80 91 91 88 85 94 93 94 93 95 93 95 94 94 88 88 80 88 82 84 90 90 89 89 89 88 89 91 87 90 87 89 90 91 93 93 91 74 83 87 89 83 92 91 89 85 93 93 94 93 93 92 96 93 93 94 87 87 80 87 82 83 89 89 88 88 88 88 89 89 86 90 86 88 89 90 92 92 90 75 83 87 89 81 91 92 88 84 92 92 93 92 93 91 93 92 93 94 96 90 90 86 87 84 86 93 93 92 92 92 93 90 95 89 93 88 91 93 92 93 87 94 73 88 89 88 85 93 89 89 91 86 91 89 86 87 88 87 88 87 87 88 87 87 88 83 86 83 84 91 91 90 90 90 91 88 91 85 90 86 88 91 90 90 85 92 72 85 88 86 84 90 86 87 100 85 88 88 84 85 86 85 85 84 85 85 84 91 92 92 87 90 87 88 95 95 93 93 95 94 91 95 88 92 88 90 94 94 95 90 95 75 89 92 91 87 94 91 92 89 89 92 92 88 90 90 91 90 90 90 91 90 91 89 90 90 86 89 85 88 95 95 94 94 94 94 90 96 88 93 88 92 94 93 94 88 95 74 90 91 90 88 94 90 89 93 88 91 90 88 88 88 88 88 87 88 89 87 96 93 93 90 91 87 91 86 87 94 94 95 95 93 94 89 94 87 91 88 90 94 93 93 87 94 77 88 90 89 85 92 89 89 91 89 91 90 87 88 88 88 88 87 88 89 87 93 91 93 93 89 89 83 85 84 87 89 89 88 88 90 89 92 91 94 94 95 94 90 91 92 87 92 72 85 89 86 84 92 87 93 87 86 90 89 85 88 89 89 90 86 87 88 87 90 87 90 89 89 91 92 86 89 86 88 95 95 94 94 94 95 91 96 88 94 88 92 95 94 95 89 97 74 90 92 90 89 95 91 91 93 88 92 91 87 89 90 89 88 88 89 90 89 95 93 93 97 92 90 92 92 88 92 88 89 97 97 96 96 98 97 91 97 88 93 88 91 96 96 97 91 96 76 90 93 91 88 94 91 91 91 91 94 94 89 90 92 92 91 90 91 92 91 93 91 95 95 95 91 95 89 90 85 86 82 85 91 91 89 89 89 89 95 93 92 97 92 96 90 91 93 90 93 74 85 89 88 86 97 91 91 88 87 90 89 90 92 90 92 93 90 90 90 89 91 88 91 91 90 92 92 91 89 90 83 85 83 86 89 89 89 89 89 89 93 92 95 96 95 95 90 91 92 88 92 73 85 89 86 83 94 88 92 87 87 90 90 87 90 91 90 92 88 89 89 88 91 87 90 89 89 97 90 91 95 92 93 85 88 88 90 93 93 92 92 94 93 91 93 89 92 89 90 94 95 93 88 93 74 89 93 88 87 92 89 96 88 88 92 91 86 87 89 90 88 88 88 89 88 90 88 93 91 91 91 93 94 90 90 88 89 84 84 83 86 90 90 89 89 89 90 93 92 94 96 96 97 90 91 91 86 92 72 85 89 88 86 94 88 91 88 86 89 88 86 88 88 88 90 87 88 88 87 91 88 90 91 89 96 91 90 95 96 90 86 86 80 82 80 83 85 85 85 85 85 85 91 88 93 93 92 92 86 87 88 84 88 71 82 85 83 80 90 85 90 83 84 87 86 83 86 87 86 88 85 85 85 84 87 83 86 85 85 93 86 86 91 97 87 92 83 84 81 82 81 84 89 89 88 88 89 89 81 88 78 84 78 82 88 88 86 81 88 68 89 85 83 83 86 83 82 84 81 82 83 77 79 81 80 79 78 80 80 79 84 84 85 88 86 82 88 89 81 80 86 81 76 92 92 89 90 88 88 97 97 95 95 95 96 90 96 87 93 87 91 95 94 95 89 96 76 91 93 91 87 94 91 91 91 89 92 92 87 88 89 89 88 88 89 89 88 93 91 94 95 94 90 95 96 90 90 93 90 86 90 92 91 89 90 88 88 96 96 94 94 94 95 91 95 89 93 89 91 95 94 95 90 95 75 90 92 91 87 94 91 92 90 89 94 92 88 89 91 91 89 89 90 90 89 92 90 93 93 92 91 94 95 91 91 93 91 87 86 97 94 94 90 92 91 90 97 97 96 96 97 98 92 96 90 94 90 92 96 97 96 90 97 76 91 94 92 89 95 92 94 92 90 94 94 89 90 91 91 90 89 91 91 90 93 92 95 95 94 92 95 97 92 92 95 92 88 89 98 98 93 92 89 90 89 89 96 96 95 95 95 96 91 95 89 93 88 90 95 95 95 89 95 74 90 92 91 88 94 91 92 90 89 92 92 87 88 90 89 88 87 89 89 88 92 90 94 94 93 90 94 96 90 90 94 90 87 89 96 96 98 93 93 89 91 91 89 95 95 94 94 95 95 90 93 88 91 88 89 95 95 95 90 94 75 88 93 91 85 92 90 92 88 90 93 92 87 89 90 91 89 89 90 91 90 91 88 95 92 93 91 92 96 90 90 93 89 86 85 95 94 97 95 91 90 86 88 87 88 95 95 95 95 95 96 87 94 85 91 85 88 94 94 92 87 94 73 92 91 89 88 92 89 89 89 87 89 90 85 86 87 87 85 84 86 87 86 90 89 91 94 92 88 93 95 88 87 92 88 83 93 95 93 95 95 92 89 89 91 88 85 85 94 94 92 92 93 93 89 93 87 91 87 90 92 93 94 87 93 75 88 90 90 86 92 90 89 89 87 89 89 86 87 87 87 87 87 87 88 87 91 89 97 92 92 88 93 94 90 88 90 89 85 85 93 92 94 92 92 90 93 93 87 90 89 91 96 96 95 95 96 97 89 95 87 92 87 90 96 96 94 89 95 75 92 93 90 88 93 90 92 90 89 92 92 86 88 89 90 88 87 89 89 89 91 90 94 94 93 90 94 96 89 89 94 89 85 94 96 95 97 96 96 97 91 93 92 85 89 87 90 94 94 93 93 94 94 91 94 90 92 90 91 94 95 93 88 94 73 92 93 88 89 92 89 94 91 88 91 92 86 88 88 89 88 87 87 89 88 91 91 92 93 92 92 93 94 90 91 93 91 88 89 94 94 95 94 92 95 91 95 70 75 80 85 90 95 100 Figure 2: Similarity of MTEB datasets. We use the best model on MTEB STS (ST5-XXL, see Table 1) to embed 100 samples for each dataset. Cosine similarities between the averaged embeddings are computed and visualized.

Retrieval Each dataset consists of a corpus, queries and a mapping for each query to relevant documents from the corpus. The aim is to ﬁnd these relevant documents. The provided model is used to embed all queries and all corpus documents and similarity scores are computed using cosine simi- larity. After ranking the corpus documents for each query based on the scores, nDCG@k, MRR@k, MAP@k, precision@k and recall@k are computed for several values of k. nDCG@10 serves as the main metric. MTEB reuses datasets and evaluation from BEIR (Thakur et al., 2021).

Semantic Textual Similarity (STS) Given a sentence pair the aim is to determine their simi- larity. Labels are continuous scores with higher numbers indicating more similar sentences. The provided model is used to embed the sentences and their similarity is computed using various distance metrics. Distances are benchmarked with ground truth similarities using Pearson and Spearman cor- relations. Spearman correlation based on cosine similarity serves as the main metric (Reimers et al., 2016).

Summarization A set of human-written and machine-generated summaries are provided. The aim is to score the machine summaries. The pro- vided model is ﬁrst used to embed all summaries.

For each machine summary embedding, distances to all human summary embeddings are computed.

The closest score (e.g. highest cosine similarity) is kept and used as the model’s score of a single machine-generated summary. Pearson and Spear- man correlations with ground truth human assess- ments of the machine-generated summaries are computed. Like for STS, Spearman correlation based on cosine similarity serves as the main met- ric (Reimers et al., 2016).

3.3 Datasets To further the diversity of MTEB, datasets of vary- ing text lengths are included.

All datasets are grouped into three categories: Sentence to sentence (S2S) A sentence is com- pared with another sentence. An example of S2S are all current STS tasks in MTEB, where the simi- larity between two sentences is assessed.

Class.

Clust.

PairClass.

Rerank.

Retr.

STS Summ.

Avg.

Num. Datasets (→) 12 11 3 4 15 10 1 56 Self-supervised methods Glove 57.29 27.73 70.92 43.29 21.62 61.85 28.87 41.97 Komninos 57.65 26.57 72.94 44.75 21.22 62.47 30.49 42.06 BERT 61.66 30.12 56.33 43.44 10.59 54.36 29.82 38.33 SimCSE-BERT-unsup 62.50 29.04 70.33 46.47 20.29 74.33 31.15 45.45 Supervised methods SimCSE-BERT-sup 67.32 33.43 73.68 47.54 21.82 79.12 23.31 48.72 coCondenser-msmarco 64.71 37.64 81.74 51.84 32.96 76.47 29.50 52.35 Contriever 66.68 41.10 82.53 53.14 41.88 76.51 30.36 56.00 SPECTER 52.37 34.06 61.37 48.10 15.88 61.02 27.66 40.28 LaBSE 62.71 29.55 78.87 48.42 18.99 70.80 31.05 45.21 LASER2 53.65 15.28 68.86 41.44 7.93 55.32 26.80 33.63 MiniLM-L6 63.06 42.35 82.37 58.04 41.95 78.90 30.81 56.26 MiniLM-L12 63.21 41.81 82.41 58.44 42.69 79.80 27.90 56.53 MiniLM-L12-multilingual 64.30 37.14 78.45 53.62 32.45 78.92 30.67 52.44 MPNet 65.07 43.69 83.04 59.36 43.81 80.28 27.49 57.78 MPNet-multilingual 67.91 38.40 80.81 53.80 35.34 80.73 31.57 54.71 OpenAI Ada Similarity 70.44 37.52 76.86 49.02 18.36 78.60 26.94 49.52 SGPT-125M-nli 61.46 30.95 71.78 47.56 20.90 74.71 30.26 45.97 SGPT-5.8B-nli 70.14 36.98 77.03 52.33 32.34 80.53 30.38 53.74 SGPT-125M-msmarco 60.72 35.79 75.23 50.58 37.04 73.41 28.90 51.23 SGPT-1.3B-msmarco 66.52 39.92 79.58 54.00 44.49 75.74 25.44 56.11 SGPT-2.7B-msmarco 67.13 39.83 80.65 54.67 46.54 76.83 27.87 57.12 SGPT-5.8B-msmarco 68.13 40.35 82.00 56.56 50.25 78.10 24.75 58.81 SGPT-BLOOM-7.1B-msmarco 66.19 38.93 81.90 55.65 48.21 77.74 24.99 57.44 GTR-Base 65.25 38.63 83.85 54.23 44.67 77.07 29.67 56.19 GTR-Large 67.14 41.60 85.33 55.36 47.42 78.19 29.50 58.28 GTR-XL 67.11 41.51 86.13 55.96 47.96 77.80 30.21 58.42 GTR-XXL 67.41 42.42 86.12 56.65 48.48 78.38 30.64 58.97 ST5-Base 69.81 40.21 85.17 53.09 33.63 81.14 31.39 55.27 ST5-Large 72.31 41.65 84.97 54.00 36.71 81.83 29.64 57.06 ST5-XL 72.84 42.34 86.06 54.71 38.47 81.66 29.91 57.87 ST5-XXL 73.42 43.71 85.06 56.43 42.24 82.63 30.08 59.51 Table 1: Average of the main metric (see Section 3.2) per task per model on MTEB English subsets.

Paragraph to paragraph (P2P) A paragraph is compared with another paragraph. MTEB imposes no limit on the input length, leaving it up to the models to truncate if necessary. Several clustering tasks are framed as both S2S and P2P tasks. The former only compare titles, while the latter include both title and content. For ArxivClustering, for example, abstracts are concatenated to the title in the P2P setting.

Sentence to paragraph (S2P) A few retrieval datasets are mixed in a S2P setting. Here a query is a single sentence, while documents are long paragraphs consisting of multiple sentences.

Similarities across 56 MTEB datasets are vi- sualized in Figure 2.

Several datasets rely on the same corpora, such as ClimateFEVER and FEVER, resulting in a score of 1. Clusters of simi- lar datasets can be seen among CQADupstack vari- ations and STS datasets. S2S and P2P variations of the same dataset tend to also be similar. Scientiﬁc datasets, such as SciDocsRR, SciFact, ArxivClus- tering, show high similarities among each other even when coming from different tasks (Reranking, Retrieval and Clustering in this case).

4 Results 4.1 Models We evaluate on the test splits of all datasets except for MSMARCO, where the dev split is used follow- ing Thakur et al. (2021). We benchmark models claiming state-of-the-art results on various embed- 0.1B 1B 2B 4B Model Parameters (Billions) 0.62 0.64 0.66 0.68 0.70 0.72 0.74 Average Performance (accuracy) Classification 0.1B 1B 2B 4B Model Parameters (Billions) 0.36 0.37 0.38 0.39 0.40 0.41 0.42 0.43 0.44 Average Performance (v_measure) Clustering 0.1B 1B 2B 4B Model Parameters (Billions) 0.76 0.78 0.80 0.82 0.84 0.86 Average Performance (ap) PairClassification 0.1B 1B 2B 4B Model Parameters (Billions) 0.51 0.52 0.53 0.54 0.55 0.56 Average Performance (map) Reranking 0.1B 1B 2B 4B Model Parameters (Billions) 0.350 0.375 0.400 0.425 0.450 0.475 0.500 Average Performance (nDCG@10) Retrieval 0.1B 1B 2B 4B Model Parameters (Billions) 0.74 0.76 0.78 0.80 0.82 Average Performance (cos. sim. spearman corr.) STS GTR ST5 SGPT Figure 3: MTEB performance scales with model size. The smallest SGPT variant underperforms similar- sized GTR and ST5 variants. This may be due to the bias-only ﬁne-tuning SGPT employs, which catches up with full ﬁne-tuning only as model size and thus the number of bias parameters increases (Muennighoff, 2022). ding tasks leading to a high representation of trans- formers (Vaswani et al., 2017). We group models into self-supervised and supervised methods.

Self-supervised methods (a) Transformer- based BERT (Devlin et al., 2018) is trained using self-supervised mask and sentence prediction tasks.

By taking the mean across the sequence length (mean-pooling) the model can directly be used to produce text embeddings.

SimCSE-Unsup (Gao et al., 2021b) uses BERT as a foundation and performs additional self-supervised training.

(b) Non-transformer: Komninos (Komninos and Manandhar, 2016) and Glove (Pennington et al., 2014) are two word embedding models that directly map words to vectors. Hence, their embeddings lack context awareness, but provide signiﬁcant speed-ups.

Supervised methods The original transformer model (Vaswani et al., 2017) consists of an encoder and decoder network. Subsequent transformers often train only encoders like BERT (Devlin et al., 2018) or decoders like GPT (Radford et al., 2019).

(a) Transformer encoder methods coCon- denser (Gao and Callan, 2021), Contriever (Izac- ard et al., 2021), LaBSE (Feng et al., 2020) and SimCSE-BERT-sup (Gao et al., 2021b) are based on the pre-trained BERT model (Devlin et al., 2018). coCondenser and Contriever add a self- supervised stage prior to supervised ﬁne-tuning for a total of three training stages. LaBSE uses BERT to perform additional pre-training on par- allel data to produce a competitive bitext mining model. SPECTER (Cohan et al., 2020a) relies on the pre-trained SciBERT (Beltagy et al., 2019) vari- ant instead and ﬁne-tunes on citation graphs. GTR (Ni et al., 2021b) and ST5 (Ni et al., 2021a) are based on the encoder part of the T5 model (Raf- fel et al., 2020) and only differ in their ﬁne-tuning datasets. After additional self-supervised training, ST5 does contrastive ﬁne-tuning on NLI (Ni et al., 2021a; Gao et al., 2021b) being geared towards STS tasks. Meanwhile, GTR ﬁne-tunes on MS- MARCO and focuses on retrieval tasks. MPNet and MiniLM correspond to ﬁne-tuned embedding models (Reimers and Gurevych, 2019) of the pre- trained MPNet (Song et al., 2020) and MiniLM (Wang et al., 2020) models using diverse datasets to target any embedding use case.

(b) Transformer decoder methods SGPT Bi- Encoders (Muennighoff, 2022) perform contrastive ﬁne-tuning of <0.1% of pre-trained parameters us- ing weighted-mean pooling. Similar to ST5 and GTR, SGPT-nli models are geared towards STS, while SGPT-msmarco models towards retrieval.

SGPT-msmarco models embed queries and doc- uments for retrieval with different special tokens to help the model distinguish their role. For non- retrieval tasks, we use its query representations.

We benchmark publicly available SGPT models based on GPT-NeoX (Andonian et al., 2021), GPT- J (Wang and Komatsuzaki, 2021) and BLOOM (Scao et al., 2022). Alternatively, cpt-text (Nee- lakantan et al., 2022) passes pre-trained GPT de- coders through a two-stage process using last token pooling to provide embeddings from decoders. We benchmark their models via the OpenAI Embed- dings API4.

(c) Non-transformer LASER (Heffernan et al., 2022) is the only context aware non-transformer model we benchmark, relying on an LSTM 4https://beta.openai.com/docs/guides/ embeddings 102 103 104 Speed (examples per sec) 35 40 45 50 55 60 MTEB Score LASER2 Komninos Glove SGPT-125M-nli SGPT-125M-msmarco SGPT-5.8B-nli SGPT-5.8B-msmarco MiniLM-L6 MPNet ST5-Base ST5-XXL GTR-Base GTR-XXL Contriever coCondenser-msmarco BERT SimCSE-BERT-sup SimCSE-BERT-unsup LaBSE MiniLM-L12 SPECTER Base Architecture LASER WordEmbeddings GPT MiniLM MPNet T5 BERT SciBERT Figure 4: Performance, speed, and size of produced embeddings (size of the circles) of different embedding models. Embedding sizes range from 1.2 kB (Glove / Komninos) to 16.4 kB (SGPT-5.8B) per example. Speed was benchmarked on STS15 using 1x Nvidia A100 80GB with CUDA 11.6.

(Hochreiter and Schmidhuber, 1997) instead. Simi- lar to LaBSE, the model trains on parallel data and focuses on bitext mining applications.

4.2 Analysis Based on the results in Table 1, we observe that there is considerable variability between tasks. No model claims the state-of-the-art in all seven En- glish tasks. There is even more variability in the results per dataset present in the appendix. Further, there remains a large gap between self-supervised and supervised methods. Self-supervised large lan- guage models have been able to close this gap in many natural language generation tasks (Chowd- hery et al., 2022). However, they appear to still require supervised ﬁne-tuning for competitive em- bedding performance.

We ﬁnd that performance strongly correlates with model size, see Figure 3.

A majority of MTEB tasks are dominated by multi-billion param- eter models. However, these come at a signiﬁcant cost as we investigate in Section 4.3.

Classiﬁcation ST5 models dominate the classiﬁ- cation task across most datasets, as can be seen in detail in the full results in the appendix. ST5-XXL has the highest average performance, 3% ahead of the best non-ST5 model, OpenAI Ada Similarity.

Clustering Despite being almost 50x smaller, the MPNet embedding model is on par with the ST5- XXL state-of-the-art on Clustering. This may be due to the large variety of datasets MPNet (and MiniLM) has been ﬁne-tuned on. Clustering re- quires coherent distances between a large number of embeddings. Models like SimCSE-sup or SGPT- nli, which are only ﬁne-tuned on a single dataset, NLI, may produce incoherent embeddings when encountering topics unseen during ﬁne-tuning. Re- latedly, we ﬁnd that the query embeddings of SGPT- msmarco and the Ada Search endpoint are competi- tive with SGPT-nli and the Ada Similarity endpoint, respectively. We refer to the public leaderboard5 for Ada Search results. This could be due to the MSMARCO dataset being signiﬁcantly larger than NLI. Thus, while the OpenAI docs recommend us- ing the similarity embeddings for clustering use cases6, the retrieval query embeddings may be the better choice in some cases.

5https://huggingface.co/spaces/mteb/l eaderboard 6https://beta.openai.com/docs/guides/ embeddings/similarity-embeddings deu-eng mal-eng nob-eng spa-eng epo-eng tur-eng tel-eng pol-eng vie-eng hrv-eng ron-eng hin-eng glg-eng sqi-eng ces-eng est-eng hun-eng slk-eng lit-eng fin-eng afr-eng tha-eng nld-eng slv-eng tgl-eng mon-eng lvs-eng dan-eng swe-eng zsm-eng cat-eng jpn-eng ina-eng ell-eng cmn-eng kat-eng eus-eng bel-eng aze-eng bos-eng fra-eng isl-eng pes-eng bul-eng nno-eng srp-eng por-eng hye-eng ukr-eng gle-eng rus-eng ind-eng mkd-eng urd-eng ita-eng mar-eng uig-eng cym-eng xho-eng heb-eng amh-eng kor-eng ast-eng wuu-eng yue-eng ido-eng fry-eng tam-eng ara-eng yid-eng ben-eng kaz-eng fao-eng tat-eng gla-eng ile-eng swh-eng uzb-eng kur-eng lat-eng jav-eng cbk-eng nds-eng khm-eng arz-eng tuk-eng nov-eng awa-eng lfn-eng hsb-eng oci-eng dsb-eng pms-eng ceb-eng max-eng war-eng swg-eng ang-eng tzl-eng csb-eng gsw-eng arq-eng orv-eng cha-eng mhr-eng bre-eng kzj-eng dtp-eng pam-eng cor-eng ber-eng kab-eng 0.0 0.2 0.4 0.6 0.8 1.0 F1 score LaBSE LASER2 MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco (a) Bitext Mining on Tatoeba en hi zh-CNpt id es th it fr ru de fa sv vi zh-TWnl ms da pl tr sq el ro hu sl ko fi ja nb ml lv he ur bn ar te af ta hy my az mn is kn tl jv sw ka km am cy zh 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Accuracy (b) Multilingual Classiﬁcation ko fr es en ar it zh ru tr de pl 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Cos. Sim. Spearman Corr. fr-en en-ar en-de it-en es-en nl-en pl-en zh-en en-tr es-it fr-pl de-fr de-en de-pl (c) Multi- and Crosslingual STS Figure 5: MTEB multilingual performance. Bitext mining is dominated by LaBSE, while classiﬁcation and STS results are mixed. SGPT-BLOOM-7B1-msmarco tends to perform well on the languages BLOOM has been pre- trained on, such as Chinese, French and Portuguese.

Pair Classiﬁcation GTR-XL and GTR-XXL have the strongest performance. Pair classiﬁca- tion is closest to STS in its framing, yet models rank signiﬁcantly differently on the two tasks. This highlights the importance of benchmarking on a diverse set of tasks to avoid blindly reusing a model for a different task.

Reranking MPNet and MiniLM models perform strongly on reranking tasks. On SciDocsRR (Co- han et al., 2020a) they perform far better than big- ger models, which is likely due to parts of Sci- DocsRR being included in their training data. Our scale of experiments and that of model pre-training make controlling for data contamination challeng- ing. Thus, we ignore overlap of MTEB datasets with model training datasets in MTEB scores. As long as enough datasets are averaged, we believe these effects to be insigniﬁcant.

Retrieval SGPT-5.8B-msmarco is the best em- bedding model on the BEIR subset in MTEB as well as on the full BEIR benchmark (Thakur et al., 2021; Muennighoff, 2022). The even larger 7.1B SGPT model making use of BLOOM (Scao et al., 2022) performs signiﬁcantly weaker, which is likely due to the multilinguality of BLOOM.

Models geared towards STS (SimCSE, ST5, SGPT- nli) perform badly on retrieval tasks. Retrieval tasks are unique in that there are two distinct types of texts: Queries and documents (“asymmetric”), while other tasks only have a single type of text (“symmetric”).

On the QuoraRetrieval dataset, which has been shown to be largely symmetric (Muennighoff, 2022), the playing ﬁeld is more even with SGPT-5.8B-nli outperforming SGPT- 5.8B-msmarco, see Table 11.

STS & Summarization Retrieval models (GTR, SGPT-msmarco) perform badly on STS, while ST5- XXL has the highest performance. This highlights the bifurcation of the ﬁeld into separate embedding models for retrieval (asymmetric) and similarity (symmetric) use cases (Muennighoff, 2022).

4.3 Efﬁciency We investigate the latency-performance trade-off of models in Figure 4. The graph allows for signiﬁ- cant elimination of model candidates in the model selection process. It brings model selection down to three clusters: Maximum speed Word Embedding models offer maximum speed with Glove taking the lead on both performance and speed, thus making the choice simple in this case.

Maximum performance If latency is less impor- tant than performance, the left-hand side of the graph offers a cluster of highly performant, but slow models. Depending on the task at hand, GTR- XXL, ST5-XXL or SGPT-5.8B may be the right choice, see Section 4.2. SGPT-5.8B comes with the additional caveat of its high-dimensional em- beddings requiring more storage.

Speed and performance The ﬁne-tuned MPNet and MiniLM models lead the middle cluster mak- ing the choice easy.

4.4 Multilinguality MTEB comes with 10 multilingual datasets across bitext mining, classiﬁcation and STS tasks. We in- vestigate performance on these in Figure 5. Tabular results can be found in Tables 12, 13 and 14.

Bitext Mining LaBSE (Feng et al., 2020) per- forms strongly across a wide array of languages in bitext mining. Meanwhile, LASER2 shows high variance across different languages. While there are additional language-speciﬁc LASER2 models available for some of the languages we benchmark, we use the default multilingual LASER2 model for all languages. This is to provide a fair one-to- one comparison of models. In practice, however, the high variance of LASER2’s performance may be resolved by mixing its model variants. MP- Net, MiniLM and SGPT-BLOOM-7B1-msmarco perform poorly on languages they have not been pre-trained on, such as German for the latter.

Classiﬁcation & STS On multilingual classiﬁ- cation and STS, the multilingual MPNet provides the overall strongest performance. It outperforms the slightly faster multilingual MiniLM on almost all languages.

Both models have been trained on the same languages, thus bringing decision- making down to performance vs speed. SGPT- BLOOM-7B1-msmarco provides state-of-the-art performance on languages like Hindi, Portuguese, Chinese or French, which the model has seen ex- tensively during pre-training. It also performs com- petitively on languages like Russian or Japanese that unintentionally leaked into its pre-training data (Muennighoff et al., 2022). However, it is not much ahead of the much cheaper MPNet. LASER2 performs consistently worse than other models.

5 Conclusion In this work, we presented the Massive Text Em- bedding Benchmark (MTEB). Consisting of 8 text embedding tasks with up to 15 datasets each and covering 112 languages, MTEB aims to provide re- liable embedding performance estimates. By open- sourcing MTEB alongside a leaderboard, we pro- vide a foundation for further pushing the state-of- the-art of available text embeddings.

To introduce MTEB, we have conducted the most comprehensive benchmarking of text embed- dings to date. Through the course of close to 5,000 experiments on over 30 different models, we have set up solid baselines for future research to build on. We found model performance on different tasks to vary strongly with no model claiming state-of- the-art on all tasks. Our studies on scaling behav- ior, model efﬁciency and multilinguality revealed various intricacies of models that should ease the decision-making process for future research or in- dustry applications of text embeddings.

We welcome task, dataset or metric contributions to the MTEB codebase7 as well as additions to the leaderboard via our automatic submission format8.

7https://github.com/embeddings-benchm ark/mteb 8https://huggingface.co/spaces/mteb/l eaderboard