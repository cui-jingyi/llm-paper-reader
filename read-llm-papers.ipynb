{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851e4669",
   "metadata": {},
   "source": [
    "## 0. Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e477b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "with open('openai_api_key.txt', 'r') as file:\n",
    "    os.environ['OPENAI_API_KEY'] = file.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11411f",
   "metadata": {},
   "source": [
    "## 1. Basic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67280323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    # Read PDF and extract text\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def process_text(text):\n",
    "    # Clean and segment text\n",
    "    \n",
    "    # Remove contents after Acknowledgments\n",
    "    ack_index = text.lower().find(\"acknowledgments\\n\")\n",
    "    if ack_index != -1:\n",
    "        text = text[:ack_index]\n",
    "    \n",
    "    # Remove any lines that are entirely whitespace\n",
    "    cleaned_text = \"\\n\".join([line for line in text.split('\\n') if line.strip()])\n",
    "    \n",
    "    # Clean sentences\n",
    "    subsentences = re.sub(r'\\n(?=[a-z])', ' ', cleaned_text)\n",
    "    sentences = re.sub(r'(?<=[^\\.!?])\\n', ' ', subsentences)\n",
    "    \n",
    "    # Combine sentences into paragraphs\n",
    "    paragraphs = sentences.split('\\n')\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "def query_openai_api(question, context):\n",
    "    \"\"\"\n",
    "    Sends a request to the OpenAI API with the given text.\n",
    "    \n",
    "    :param question: The question you want to ask based on the context.\n",
    "    :param context: The context (e.g., a paragraph from the PDF) related to the question.\n",
    "    :param api_key: OpenAI API key.\n",
    "    :return: Response from the API.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \n",
    "             \"content\": \"You are an assistant skilled in analyzing and summarizing academic articles.\"},\n",
    "            {\"role\": \"user\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=2000\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa71e3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MTEB: Massive Text Embedding Benchmark Niklas Muennighoff1, Nouamane Tazi1, Loïc Magne1, Nils Reimers2* 1Hugging Face 2cohere.ai 1firstname@hf.co 2info@nils-reimers.de Abstract Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art em- beddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the ﬁeld difﬁcult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we intro- duce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks cov- ering a total of 58 datasets and 112 languages.', 'Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date.', 'We ﬁnd that no particular text embedding method dominates across all tasks. This suggests that the ﬁeld has yet to converge on a universal text embedding method and scale it up sufﬁciently to provide state-of-the-art results on all embed- ding tasks.']\n",
      "['To introduce MTEB, we have conducted the most comprehensive benchmarking of text embed- dings to date. Through the course of close to 5,000 experiments on over 30 different models, we have set up solid baselines for future research to build on. We found model performance on different tasks to vary strongly with no model claiming state-of- the-art on all tasks. Our studies on scaling behav- ior, model efﬁciency and multilinguality revealed various intricacies of models that should ease the decision-making process for future research or in- dustry applications of text embeddings.', 'We welcome task, dataset or metric contributions to the MTEB codebase7 as well as additions to the leaderboard via our automatic submission format8.', '7https://github.com/embeddings-benchm ark/mteb 8https://huggingface.co/spaces/mteb/l eaderboard']\n"
     ]
    }
   ],
   "source": [
    "filepath = 'import/MTEB.pdf'\n",
    "\n",
    "text = read_pdf(filepath)\n",
    "paras = process_text(text)\n",
    "\n",
    "to_save = \"\\n\\n\".join(paras)\n",
    "with open('data/MTEB.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(to_save)\n",
    "\n",
    "print(paras[:3])\n",
    "print(paras[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a03f6",
   "metadata": {},
   "source": [
    "### Sample Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ec9aed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The title of the paper is \"MTEB: Massive Text Embedding Benchmark\".'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the title of the paper?\"\n",
    "query_openai_api(question, paras[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d03f7",
   "metadata": {},
   "source": [
    "## 2. Llama-Index Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a244d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now more complex questions requiring finding relevant chunks from the text based on STS\n",
    "# Let's leverage llama-index for this\n",
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "# Load data\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# Parse nodes\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=10) # Could implement paragraph chuking\n",
    "service_context = ServiceContext.from_defaults(node_parser=node_parser)\n",
    "\n",
    "# Index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k = 5) # Returnn top 5 most similar documents\n",
    "\n",
    "def answer(question):\n",
    "    response = query_engine.query(question)\n",
    "    return print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da4d03",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79869d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of this paper is \"Massive Text Embedding Benchmark\".\n"
     ]
    }
   ],
   "source": [
    "answer(\"What's the name of this paper? Note that the title of this paper is usually the title at the beginning of the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea5bc74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niklas Muennighoff, Nouamane Tazi, Loïc Magne, Nils Reimers\n"
     ]
    }
   ],
   "source": [
    "answer(\"What are the full names of the authors of this paper? Note that the authors are the names immediately following the paper title.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd0ea038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper discusses the evaluation of different embedding models on various NLP tasks. It highlights the strengths and weaknesses of different models for tasks such as retrieval, clustering, and semantic textual similarity (STS). The benchmark used in the evaluation, called MTEB, consists of 58 datasets across 8 task types. The paper emphasizes the importance of benchmarks and evaluation frameworks in driving NLP progress. It also mentions the insufficiency of existing benchmarks like SentEval and introduces the USEB benchmark that focuses on reranking tasks. The paper aims to make it easy to reproduce the results by providing versioning at the dataset and software level. Overall, the paper provides insights into the performance of different embedding models and aims to simplify the process of selecting the right model for specific NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "answer(\"Can you summarize key takeaways from this paper?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-hack",
   "language": "python",
   "name": "llm-hack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
